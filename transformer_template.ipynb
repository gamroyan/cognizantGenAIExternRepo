{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c7d2897",
   "metadata": {},
   "source": [
    "# Transformer Text Generation\n",
    "\n",
    "In this notebook, we will explore how transformer models (like GPT-2) can generate text based on a given prompt. We will experiment with generating text by adjusting parameters like temperature and sequence length.\n",
    "\n",
    "## Instructions\n",
    "1. Change the prompt below to experiment with different types of text generation.\n",
    "2. Adjust the `max_length` and `temperature` parameters to see how they affect the output.\n",
    "3. Generate at least 3 samples with different prompts and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dbce095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading books vs watching movies\n",
      "\n",
      "The best way to learn to read is to read books. I have read a lot of books, but I have never read a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load GPT-2 text generation model\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Set your prompt\n",
    "prompt = 'Reading books vs watching movies'\n",
    "\n",
    "# Generate text\n",
    "result = generator(prompt, max_length=50, temperature=0.2)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c69a033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You work at an office. One day there was a problem with the staff that had to be fixed. We had a group of employees at the office, and we had to do an investigation. We found out that there were people who were working at this office at the time. We found out that there were people who were working at the office at the time, and we were able to get this person out of the office at the time. We found out that there were people who were working at this office at the time.\n",
      "\n",
      "Q: So you were in the office at the time and you were fired?\n",
      "\n",
      "A: Yes, I was fired. I was fired. I was fired. I was fired.\n",
      "\n",
      "Q: So you were fired and you were fired?\n",
      "\n",
      "A: Yes, I was fired. I was fired. I was fired.\n",
      "\n",
      "Q: But you were fired and you were fired by a supervisor?\n",
      "\n",
      "A: Yes, I was fired. I was fired.\n",
      "\n",
      "Q: But you were fired by another supervisor?\n",
      "\n",
      "A: Yes, sir.\n",
      "\n",
      "Q: Were you fired by another supervisor?\n",
      "\n",
      "A: Yes, sir.\n",
      "\n",
      "Q: Do you know if there was a problem with the other team members, or did you just\n",
      "Once upon a time, there was a kingdom in the land of the free.\n",
      "\n",
      "If the first king of the peoples had not been a king, there would not exist any kings in the world.\n",
      "\n",
      "There is no need to be slaves.\n",
      "\n",
      "There is no need to be slaves when the king of a land has conquered one of the peoples.\n",
      "\n",
      "There is no need to be slaves when the King of the peoples has conquered a tribe of nations.\n",
      "\n",
      "There is no need to be slaves when the King of the peoples has conquered a nation.\n",
      "\n",
      "For the sake of humanity, one should be free, and yet the oppressed peoples of the earth deserve no privileges because of their caste and race.\n",
      "\n",
      "One should not be enslaved by man.\n",
      "\n",
      "Man should not become a slave to the beast.\n",
      "\n",
      "Man should not become a slave to the beast by any means.\n",
      "\n",
      "Man should not become a slave to the beast with any power or ability.\n",
      "\n",
      "Man should not become a slave to the beast if he takes a vow of war with the beast.\n",
      "\n",
      "Man should not become a slave to the beast if he is subjected to any punishment, whether for life or for loss of possessions.\n",
      "\n",
      "Man should not become a slave to the beast if he is tortured\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different prompts\n",
    "prompt = 'You work at an office. One day there was a problem'\n",
    "result = generator(prompt, max_length=50, temperature=0.5)\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "prompt = 'Once upon a time, there was a kingdom'\n",
    "result = generator(prompt, max_length=100, temperature=0.8)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d0d32",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "Transformers are a neural network architecture designed to handle sequential data like text, speech, and images. Their key characteristic is the self-attention mechanism, which allows the model to focus on different parts of a sequence simultaneousy as opposed to one step at a time. Regardless of position, the self-attention mechanism identifies relationships across the whole sequence. Transformers are important in AI because they've changed and improved how models process and understand data. Their use of self-attention is unlike any past RNN or LSTM, which makes it the most versatile and efficient architecture in AI right now.\n",
    "\n",
    "**Experiment Summary**\n",
    "\n",
    "I used the GPT-2 Transformer model provided to us through the transformer template in the assignment writeup. I had an easy time setting it up and playing around with different kinds of prompts and temperatures. I tested a variety of prompts, like sentence starters, either/or questions, and short open-ended inputs that gave the GPT more variability. For example, I tried sentence starters like \"Once upon a time...\" and \"The future of AI is...\" to see how creative the model could be. I also asked it to make choices, like \"reading books or watching movies,\" and also tested shorter inputs that allowed it to be more creative, like \"describe a fruit that doesn't exist,\" and \"invent a new holiday.\" Even running the GPT again over the same prompt and temperature generated very different responses.\n",
    "\n",
    "**Observations**\n",
    "\n",
    "I have three different outputs I generated above. In the first code block, I tried to get it to make a choice by asking it \"reading books or watching movies,\" but set the temperature very low to 0.2. This genererated something very dull and plain. I realized that the temperature decides the randomness and how creative the model can be. With a low temperature like 0.2, the GPT responded to this question very straightforwardly. It said that it has read many books, and kept repeating \"I have never watched a movie\" until it hit the max_length of 50. This was a very uncanny response. Opposed to this, in the lower code block, I gave to prompts with higher temperatures that allowed the GPT to be more random and creative. With a temperature of 0.5, it still generated something very predictible, but with a little bit more variability. Increasing the temperature to 0.8 in the last prompt generated a very creative and unpredictible response. Although there's still some repetition in the output, the model entirely created a unique piece of work based on the prompt.\n",
    "\n",
    "**Reflection**\n",
    "\n",
    "I learned that Transformers generate text by predicting the next word in a sequence based on the context of all the previous words. They use self-attention to weigh which parts of the input are most relevant, which allowes them to evaluate all parts of a sentence at once. This helps them maintain consistency and structure across longer outputs when generating texts. I was surprised by how creative the model could be, especially when given open-ended or imaginative prompts. Even without changing the temperature, the GPT was able to come up with different outputs for a single prompt. Changing the value of the temperature can be valuable when working with specific types of data. For example, when analyzing numerical data, a low temperature would produce only relevant outputs, which is what you'd need when doing research. On the other hand, a higher temperature produces very creative results, which would be very helpful when trying to brainstorm new ideas. \n",
    "\n",
    "However, I noticed a few limitations, like how the model sometimes repeated itself or lost coherence in longer outputs. It could also make up facts without any awareness when the temperature was too high. I noticed that it doesn’t really understand meaning the way humans do. It's obvious that it mimics language patterns based on probabilities, so while it sounds convincing, it doesn’t actually “know” what it's saying. Also, when prompts were vague or too short, (like \"watching movies or reading books\") the model sometimes defaulted to generic or overly formal responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
