{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c7d2897",
   "metadata": {},
   "source": [
    "# Transformer Text Generation\n",
    "\n",
    "In this notebook, we will explore how transformer models (like GPT-2) can generate text based on a given prompt. We will experiment with generating text by adjusting parameters like temperature and sequence length.\n",
    "\n",
    "## Instructions\n",
    "1. Change the prompt below to experiment with different types of text generation.\n",
    "2. Adjust the `max_length` and `temperature` parameters to see how they affect the output.\n",
    "3. Generate at least 3 samples with different prompts and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dbce095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading books vs watching movies\n",
      "\n",
      "The best way to learn to read is to read books. I have read a lot of books, but I have never read a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have never watched a movie. I have\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load GPT-2 text generation model\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Set your prompt\n",
    "prompt = 'Reading books vs watching movies'\n",
    "\n",
    "# Generate text\n",
    "result = generator(prompt, max_length=50, temperature=0.2)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c69a033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You work at an office. One day there was a problem with the staff that had to be fixed. We had a group of employees at the office, and we had to do an investigation. We found out that there were people who were working at this office at the time. We found out that there were people who were working at the office at the time, and we were able to get this person out of the office at the time. We found out that there were people who were working at this office at the time.\n",
      "\n",
      "Q: So you were in the office at the time and you were fired?\n",
      "\n",
      "A: Yes, I was fired. I was fired. I was fired. I was fired.\n",
      "\n",
      "Q: So you were fired and you were fired?\n",
      "\n",
      "A: Yes, I was fired. I was fired. I was fired.\n",
      "\n",
      "Q: But you were fired and you were fired by a supervisor?\n",
      "\n",
      "A: Yes, I was fired. I was fired.\n",
      "\n",
      "Q: But you were fired by another supervisor?\n",
      "\n",
      "A: Yes, sir.\n",
      "\n",
      "Q: Were you fired by another supervisor?\n",
      "\n",
      "A: Yes, sir.\n",
      "\n",
      "Q: Do you know if there was a problem with the other team members, or did you just\n",
      "Once upon a time, there was a kingdom in the land of the free.\n",
      "\n",
      "If the first king of the peoples had not been a king, there would not exist any kings in the world.\n",
      "\n",
      "There is no need to be slaves.\n",
      "\n",
      "There is no need to be slaves when the king of a land has conquered one of the peoples.\n",
      "\n",
      "There is no need to be slaves when the King of the peoples has conquered a tribe of nations.\n",
      "\n",
      "There is no need to be slaves when the King of the peoples has conquered a nation.\n",
      "\n",
      "For the sake of humanity, one should be free, and yet the oppressed peoples of the earth deserve no privileges because of their caste and race.\n",
      "\n",
      "One should not be enslaved by man.\n",
      "\n",
      "Man should not become a slave to the beast.\n",
      "\n",
      "Man should not become a slave to the beast by any means.\n",
      "\n",
      "Man should not become a slave to the beast with any power or ability.\n",
      "\n",
      "Man should not become a slave to the beast if he takes a vow of war with the beast.\n",
      "\n",
      "Man should not become a slave to the beast if he is subjected to any punishment, whether for life or for loss of possessions.\n",
      "\n",
      "Man should not become a slave to the beast if he is tortured\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different prompts\n",
    "prompt = 'You work at an office. One day there was a problem'\n",
    "result = generator(prompt, max_length=50, temperature=0.5)\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "prompt = 'Once upon a time, there was a kingdom'\n",
    "result = generator(prompt, max_length=100, temperature=0.8)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d0d32",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "Now that you have experimented with text generation, write a brief report on your observations.\n",
    "\n",
    "1. What patterns did you notice in the generated text?\n",
    "2. How did changing the temperature affect the creativity and coherence of the text?\n",
    "3. What types of prompts yielded the most coherent results?\n",
    "4. What are the limitations of GPT-2 based on your experimentation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
